import os
import csv
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pytz
import time
import json

# ================= é…ç½®åŒºåŸŸ =================
# æ•°æ®è·å–é…ç½®
DEFAULT_DAYS = 10  # é»˜è®¤è·å–æœ€è¿‘10å¤©çš„æ•°æ®
ENABLE_PUSH = True  # æ˜¯å¦å¯ç”¨æ¶ˆæ¯æ¨é€

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"  # æ ‡å‡†åŸºç¡€URL
MODEL_NAME = "deepseek-chat"

# æ¨é€é…ç½® (WxPusher)
WXPUSHER_APP_TOKEN = os.getenv("WXPUSHER_APP_TOKEN", "AT_UHus2F8p0yjnG6XvGEDzdCp5GkwvLdkc")
WXPUSHER_TOPIC_IDS = [42540]  # ç›®æ ‡ä¸»é¢˜ ID åˆ—è¡¨
WXPUSHER_URL = "https://wxpusher.zjiecode.com/api/send/message"

# æ—¶åŒºé…ç½®
BEIJING_TZ = pytz.timezone('Asia/Shanghai')

def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(BEIJING_TZ)

# ================= æ•°æ®è·å–æ ¸å¿ƒå‡½æ•°æ¨¡å— =================

def fetch_page(date_str, zdt_type, page_num):
    """
    Fetch a single page of ZDT data.
    """
    url = "https://gateway.jrj.com/quot-dc/zdt/v1/record"
    
    headers = {
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0",
        "Referer": "https://summary.jrj.com.cn/",
        "Origin": "https://summary.jrj.com.cn",
        "deviceinfo": '{"productId":"6000021","version":"1.0.0","device":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0","sysName":"Chrome","sysVersion":["chrome/142.0.0.0"]}',
        "productId": "6000021",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6"
    }

    payload = {
        "td": date_str,
        "zdtType": zdt_type,
        "pageNum": page_num,
        "pageSize": 20
    }

    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è·å–ç¬¬ {page_num} é¡µæ—¶å‡ºé”™: {e}")
        return None

def fetch_all_jrj_data(date_str=None, zdt_type="dt"):
    """
    Fetch all pages of ZDT data.
    """
    if not date_str:
        date_str = datetime.now().strftime("%Y%m%d")
        
    all_items = []
    page_num = 1
    
    print(f"[{date_str}] æ­£åœ¨è·å– {zdt_type.upper()} æ•°æ®...")
    
    while True:
        data = fetch_page(date_str, zdt_type, page_num)
        if not data or 'data' not in data:
            break
            
        page_data = data['data']
        items = page_data.get('list', [])
        if not items:
            break

        all_items.extend(items)
        
        if not page_data.get('hasNextPage'):
            break
            
        page_num += 1
        time.sleep(0.2)  # Reduced delay for faster batch processing
        
    return all_items

def fetch_last_days(days=10, start_date_str=None):
    """
    Fetch data for the last N trading days.
    Returns all data as a dictionary instead of saving to files.
    """
    if not start_date_str:
        start_date_str = datetime.now().strftime("%Y%m%d")
        
    current_date = datetime.strptime(start_date_str, "%Y%m%d")
    days_collected = 0
    max_lookback = days * 3  # Avoid infinite loop if long holidays
    days_checked = 0
    
    print(f"æ­£åœ¨è·å–ä» {start_date_str} å¼€å§‹è¿‡å» {days} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®...")
    
    # Store all data in memory
    all_data = {}
    
    while days_collected < days and days_checked < max_lookback:
        date_str = current_date.strftime("%Y%m%d")
        
        print(f"\næ­£åœ¨å¤„ç†æ—¥æœŸ: {date_str}")
        
        # Fetch Limit Up (ZT) first as it's more likely to exist
        items_zt = fetch_all_jrj_data(date_str=date_str, zdt_type="zt")
        
        # Fetch Limit Down (DT)
        items_dt = fetch_all_jrj_data(date_str=date_str, zdt_type="dt")
        
        # If we got any data, count it as a valid day
        if items_zt or items_dt:
            all_data[f"{date_str}_zt"] = items_zt
            all_data[f"{date_str}_dt"] = items_dt
            
            days_collected += 1
            print(f"æ‰¾åˆ° {date_str} çš„æœ‰æ•ˆæ•°æ®ã€‚({days_collected}/{days})")
        else:
            print(f"æœªæ‰¾åˆ° {date_str} çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯èŠ‚å‡æ—¥/å‘¨æœ«ï¼‰ã€‚")
            
        # Go back one day
        current_date -= timedelta(days=1)
        days_checked += 1
        
        # Be polite
        time.sleep(0.5)

    print(f"\nå®Œæˆã€‚å…±æ”¶é›†äº† {days_collected} å¤©çš„æ•°æ®ã€‚")
    return all_data

# ================= æ•°æ®è·å–ä¸å¤„ç†æ¨¡å— =================

def read_csv_content(file_path):
    """è¯»å–CSVæ–‡ä»¶å†…å®¹å¹¶è¿”å›ä¸ºå­—ç¬¦ä¸²ã€‚"""
    if not os.path.exists(file_path):
        return None
    
    with open(file_path, 'r', encoding='utf-8-sig') as f:
        return f.read()

def process_memory_data_to_csv(memory_data):
    """
    å°†å†…å­˜ä¸­çš„æ•°æ®ç›´æ¥è½¬æ¢ä¸ºCSVæ ¼å¼å­—ç¬¦ä¸²
    """
    import csv
    from io import StringIO
    
    # åˆ›å»ºå†…å­˜CSVç¼“å†²åŒº
    output = StringIO()
    
    # å®šä¹‰CSVå­—æ®µï¼Œç¡®ä¿ä¸analyze_market_structureå‡½æ•°æœŸæœ›çš„å­—æ®µä¸€è‡´
    csv_fields = ['æ—¥æœŸ', 'ç±»å‹', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æŒ¯å¹…', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡', 'è¿æ¿å¤©æ•°', 'å°å•æ—¶é—´']
    
    # å†™å…¥CSVå¤´éƒ¨
    writer = csv.DictWriter(output, fieldnames=csv_fields)
    writer.writeheader()
    
    # å¤„ç†å†…å­˜æ•°æ®
    for key, items in memory_data.items():
        if items:  # ç¡®ä¿æœ‰æ•°æ®
            date_str = key.split('_')[0]  # æå–æ—¥æœŸ
            zdt_type = 'æ¶¨åœ' if 'zt' in key else 'è·Œåœ'
            
            for item in items:
                # æ„å»ºCSVè¡Œæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
                row_data = {
                    'æ—¥æœŸ': date_str,
                    'ç±»å‹': zdt_type,
                    'è‚¡ç¥¨ä»£ç ': item.get('code', ''),
                    'è‚¡ç¥¨åç§°': item.get('name', ''),
                    'æœ€æ–°ä»·': item.get('last_price', 0),
                    'æ¶¨è·Œå¹…': item.get('pct_chg', 0),
                    'æŒ¯å¹…': item.get('amp', 0),
                    'æˆäº¤é¢': item.get('amt', 0),
                    'æ¢æ‰‹ç‡': item.get('turnover_rate', 0),
                    'è¿æ¿å¤©æ•°': item.get('lianban_days', 0),
                    'å°å•æ—¶é—´': item.get('order_time', '')
                }
                writer.writerow(row_data)
    
    csv_content = output.getvalue()
    output.close()
    
    # è°ƒè¯•ï¼šæ‰“å°å‰å‡ è¡ŒCSVå†…å®¹
    csv_lines = csv_content.strip().split('\n')
    print(f"[Debug] CSVå¤´éƒ¨: {csv_lines[0] if csv_lines else 'None'}")
    if len(csv_lines) > 1:
        print(f"[Debug] CSVç¬¬ä¸€è¡Œæ•°æ®: {csv_lines[1]}")
    
    return csv_content

def fetch_market_data(days=DEFAULT_DAYS):
    """
    è·å–å¸‚åœºæ•°æ®
    æ•°æ®å…¨éƒ¨é€šè¿‡ç½‘ç»œè·å–ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
    """
    print("=== ç¬¬ä¸€æ­¥ï¼šç½‘ç»œè·å–æœ€æ–°å¸‚åœºæ•°æ® ===")
    # ç›´æ¥è·å–å†…å­˜æ•°æ®ï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶
    memory_data = fetch_last_days(days=days)
    
    if not memory_data:
        print("ç½‘ç»œè·å–æ•°æ®å¤±è´¥ã€‚")
        return None, None

    print(f"\n=== ç¬¬äºŒæ­¥ï¼šå°†å†…å­˜æ•°æ®è½¬æ¢ä¸º CSV æ ¼å¼ ===")
    # ç›´æ¥å¤„ç†å†…å­˜æ•°æ®ä¸ºCSVæ ¼å¼
    csv_content = process_memory_data_to_csv(memory_data)
    
    if not csv_content:
        print("è½¬æ¢ CSV æ ¼å¼å¤±è´¥ã€‚")
        return None, None

    print(f"\n=== ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡æ•°æ®è¿›è¡Œåˆ†æ ===")
    # ä¸éœ€è¦è¯»å–æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„CSVå†…å®¹
    csv_lines = csv_content.strip().split('\n')
    print(f"[Info] æˆåŠŸè½¬æ¢ {len(csv_lines)-1} æ¡æ•°æ®è®°å½•")
    
    return "memory_data.csv", csv_content

# ================= æ•°æ®é¢„åˆ†ææ¨¡å— =================

def analyze_market_structure(csv_content, days=DEFAULT_DAYS):
    """
    å¯¹æ¶¨åœè·Œåœæ•°æ®è¿›è¡Œç»“æ„åŒ–åˆ†æ
    ä¸ºAIåˆ†æå‡†å¤‡æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ•°æ®
    """
    # è§£æCSVæ•°æ®
    lines = csv_content.strip().split('\n')
    if len(lines) < 2:
        return None
    
    headers = lines[0].split(',')
    data_lines = lines[1:]
    
    # æ„å»ºDataFrame
    data_rows = []
    for line in data_lines:
        if line.strip():
            data_rows.append(line.split(','))
    
    if not data_rows:
        return None
    
    df = pd.DataFrame(data_rows, columns=headers)
    
    # æ•°æ®ç±»å‹è½¬æ¢
    numeric_columns = ['æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æŒ¯å¹…', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡', 'è¿æ¿å¤©æ•°']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æŒ‰æ—¥æœŸåˆ†ç»„åˆ†æ
    date_groups = df.groupby('æ—¥æœŸ')
    
    analysis_result = {
        'summary': {},
        'daily_stats': {},
        'sector_analysis': {},
        'risk_analysis': {},
        'trading_heat': {}
    }
    
    # å…¨å±€ç»Ÿè®¡
    total_records = len(df)
    zt_count = len(df[df['ç±»å‹'] == 'æ¶¨åœ'])
    dt_count = len(df[df['ç±»å‹'] == 'è·Œåœ'])
    
    analysis_result['summary'] = {
        'total_records': total_records,
        'zt_count': zt_count,
        'dt_count': dt_count,
        'date_range': f"{df['æ—¥æœŸ'].min()} è‡³ {df['æ—¥æœŸ'].max()}" if not df['æ—¥æœŸ'].empty else "æœªçŸ¥"
    }
    
    # æ¯æ—¥ç»Ÿè®¡åˆ†æ
    for date, group in date_groups:
        zt_daily = len(group[group['ç±»å‹'] == 'æ¶¨åœ'])
        dt_daily = len(group[group['ç±»å‹'] == 'è·Œåœ'])
        
        # è¿æ¿åˆ†æ
        max_lianban = group['è¿æ¿å¤©æ•°'].max() if 'è¿æ¿å¤©æ•°' in group.columns else 0
        lianban_distribution = group['è¿æ¿å¤©æ•°'].value_counts().to_dict() if 'è¿æ¿å¤©æ•°' in group.columns else {}
        
        # å¹³å‡æ¶¨è·Œå¹…åˆ†æ
        avg_zt_change = group[group['ç±»å‹'] == 'æ¶¨åœ']['æ¶¨è·Œå¹…'].mean() if len(group[group['ç±»å‹'] == 'æ¶¨åœ']) > 0 else 0
        avg_dt_change = group[group['ç±»å‹'] == 'è·Œåœ']['æ¶¨è·Œå¹…'].mean() if len(group[group['ç±»å‹'] == 'è·Œåœ']) > 0 else 0
        
        analysis_result['daily_stats'][date] = {
            'zt_count': zt_daily,
            'dt_count': dt_daily,
            'max_lianban': max_lianban,
            'lianban_distribution': lianban_distribution,
            'avg_zt_change': avg_zt_change,
            'avg_dt_change': avg_dt_change
        }
    
    # æ¿å—åˆ†æï¼ˆåŸºäºè‚¡ç¥¨åç§°å…³é”®è¯ï¼‰
    sector_keywords = {
        'å†›å·¥': ['å†›å·¥', 'èˆªå¤©', 'èˆ¹èˆ¶', 'èˆªç©º', 'ä¸­èˆ¹', 'æ±Ÿé¾™', 'äºšæ˜Ÿ', 'é•¿åŸ', 'åŒ—æ–—', 'å«æ˜Ÿ', 'å¯¼å¼¹', 'é˜²åŠ¡'],
        'ç§‘æŠ€': ['ç§‘æŠ€', 'è½¯ä»¶', 'ç”µå­', 'ä¿¡æ¯', 'é€šä¿¡', 'äº’è”ç½‘', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'äººå·¥æ™ºèƒ½', 'ç®—åŠ›', 'æ•°æ®', 'äº‘è®¡ç®—', '5G', 'ç‰©è”ç½‘', 'ä¿¡åˆ›'],
        'æ¶ˆè´¹': ['é£Ÿå“', 'é¥®æ–™', 'é…’', 'æ¶ˆè´¹', 'é›¶å”®', 'ç™¾è´§', 'æœè£…', 'å®¶ç”µ', 'å®¶å±…', 'ä¹³ä¸š', 'è°ƒå‘³å“', 'å•¤é…’', 'ç™½é…’', 'é»„é…’'],
        'æ–°èƒ½æº': ['æ–°èƒ½æº', 'é”‚ç”µ', 'ç”µæ± ', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½', 'æ°¢èƒ½', 'å……ç”µæ¡©', 'é”‚çŸ¿', 'é’´', 'é•', 'ç¡…ç‰‡', 'é€†å˜å™¨', 'ç»¿ç”µ'],
        'åŒ»è¯': ['åŒ»è¯', 'åŒ»ç–—', 'ç”Ÿç‰©', 'åˆ¶è¯', 'å¥åº·', 'ç–«è‹—', 'åˆ›æ–°è¯', 'CXO', 'å™¨æ¢°', 'ä¸­è¯', 'åŒ»ç¾', 'å…»è€'],
        'é‡‘è': ['é“¶è¡Œ', 'è¯åˆ¸', 'ä¿é™©', 'é‡‘è', 'æœŸè´§', 'ä¿¡æ‰˜', 'æ”¯ä»˜', 'æ•°å­—è´§å¸', 'äº’é‡‘', 'åˆ¸å•†', 'AMC'],
        'åœ°äº§': ['åœ°äº§', 'æˆ¿äº§', 'å»ºç­‘', 'å»ºæ', 'è£…ä¿®', 'ç‰©ä¸š', 'å®¶å±…', 'æ°´æ³¥', 'é’¢é“', 'å·¥ç¨‹æœºæ¢°', 'åŸºå»º'],
        'æ±½è½¦': ['æ±½è½¦', 'æ•´è½¦', 'é›¶éƒ¨ä»¶', 'æ–°èƒ½æºæ•´è½¦', 'æ™ºèƒ½é©¾é©¶', 'è½¦è”ç½‘', 'çƒ­ç®¡ç†', 'è½»é‡åŒ–', 'ä¸€ä½“åŒ–å‹é“¸'],
        'å‘¨æœŸ': ['ç…¤ç‚­', 'æœ‰è‰²', 'åŒ–å·¥', 'çŸ³æ²¹', 'å¤©ç„¶æ°”', 'ç¨€åœŸ', 'é»„é‡‘', 'é“œ', 'é“', 'é“…é”Œ', 'é’›', 'æ°ŸåŒ–å·¥', 'ç£·åŒ–å·¥', 'åŒ–çº¤'],
        'å†œä¸š': ['å†œä¸š', 'ç§æ¤', 'å…»æ®–', 'é¥²æ–™', 'ç§å­', 'åŒ–è‚¥', 'å†œè¯', 'æ¸”ä¸š', 'çŒªè‚‰', 'é¸¡è‚‰', 'ç³–ä¸š', 'æ©¡èƒ¶'],
        'ç”µåŠ›': ['ç”µåŠ›', 'ç«ç”µ', 'æ°´ç”µ', 'æ ¸ç”µ', 'ç”µç½‘', 'ç‰¹é«˜å‹', 'ç”µåŠ›æ”¹é©', 'è™šæ‹Ÿç”µå‚'],
        'ç¯ä¿': ['ç¯ä¿', 'æ°´åŠ¡', 'å›ºåºŸ', 'å¤§æ°”æ²»ç†', 'åƒåœ¾åˆ†ç±»', 'å†ç”Ÿèµ„æº', 'åŠ¨åŠ›ç”µæ± å›æ”¶'],
        'ä¼ åª’': ['ä¼ åª’', 'æ¸¸æˆ', 'å½±è§†', 'å‡ºç‰ˆ', 'å¹¿å‘Š', 'ç›´æ’­', 'çŸ­è§†é¢‘', 'å…ƒå®‡å®™', 'NFT'],
        'æ•™è‚²': ['æ•™è‚²', 'åŸ¹è®­', 'èŒæ•™', 'é«˜æ•™', 'åœ¨çº¿æ•™è‚²'],
        'æ—…æ¸¸': ['æ—…æ¸¸', 'é…’åº—', 'é¤é¥®', 'èˆªç©º', 'æœºåœº', 'å…ç¨', 'æ™¯åŒº'],
        'ç‰©æµ': ['ç‰©æµ', 'å¿«é€’', 'èˆªè¿', 'æ¸¯å£', 'é“è·¯', 'å…¬è·¯', 'ä¾›åº”é“¾'],
        'å…¬ç”¨': ['ç‡ƒæ°”', 'ä¾›çƒ­', 'ç¯å«', 'å…¬äº¤', 'åœ°é“']
    }
    
    for sector, keywords in sector_keywords.items():
        sector_stocks = df[df['è‚¡ç¥¨åç§°'].str.contains('|'.join(keywords), na=False)]
        if len(sector_stocks) > 0:
            zt_in_sector = len(sector_stocks[sector_stocks['ç±»å‹'] == 'æ¶¨åœ'])
            dt_in_sector = len(sector_stocks[sector_stocks['ç±»å‹'] == 'è·Œåœ'])
            analysis_result['sector_analysis'][sector] = {
                'total': len(sector_stocks),
                'zt_count': zt_in_sector,
                'dt_count': dt_in_sector
            }
    
    # é£é™©åˆ†æ
    high_risk_stocks = df[
        (df['è¿æ¿å¤©æ•°'] >= 3) |  # é«˜ä½è¿æ¿
        (df['ç±»å‹'] == 'è·Œåœ') & (df['æ¶¨è·Œå¹…'] < -0.09)  # æ·±åº¦è·Œåœ
    ]
    
    analysis_result['risk_analysis'] = {
        'high_risk_count': len(high_risk_stocks),
        'high_risk_stocks': high_risk_stocks['è‚¡ç¥¨åç§°'].tolist()[:10]  # å‰10åª
    }
    
    # äº¤æ˜“çƒ­åº¦åˆ†æ
    if 'æˆäº¤é¢' in df.columns:
        df['æˆäº¤é¢'] = pd.to_numeric(df['æˆäº¤é¢'], errors='coerce')
        high_volume_stocks = df.nlargest(20, 'æˆäº¤é¢')
        analysis_result['trading_heat'] = {
            'total_volume': df['æˆäº¤é¢'].sum(),
            'high_volume_stocks': high_volume_stocks[['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æˆäº¤é¢', 'ç±»å‹']].to_dict('records')
        }
    
    return analysis_result

# ================= AI åˆ†ææ¨¡å— =================

def prepare_ai_context(csv_content, analysis_result):
    """
    ä¸ºAIå‡†å¤‡ç»“æ„åŒ–çš„åˆ†æä¸Šä¸‹æ–‡
    """
    if not analysis_result:
        return "æ•°æ®åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
    
    summary = analysis_result['summary']
    daily_stats = analysis_result['daily_stats']
    sector_analysis = analysis_result['sector_analysis']
    risk_analysis = analysis_result['risk_analysis']
    
    # è®¡ç®—CSVå¤§å°ï¼Œæ§åˆ¶æ˜¯å¦ç›´æ¥åµŒå…¥
    csv_size = len(csv_content)
    
    # æ„å»ºç»“æ„åŒ–æç¤ºè¯
    context = f"""
[å¸‚åœºæ•°æ®æ¦‚è§ˆ]
- åˆ†æå‘¨æœŸ: {summary['date_range']}
- æ€»è®°å½•æ•°: {summary['total_records']} æ¡
- æ¶¨åœè‚¡ç¥¨: {summary['zt_count']} åª
- è·Œåœè‚¡ç¥¨: {summary['dt_count']} åª

[æ¯æ—¥å¸‚åœºæ¸©åº¦è®¡]"""
    
    # æ·»åŠ æ¯æ—¥æ•°æ®
    for date, stats in daily_stats.items():
        context += f"""
- {date}: æ¶¨åœ{stats['zt_count']}åª, è·Œåœ{stats['dt_count']}åª, æœ€é«˜è¿æ¿{stats['max_lianban']}å¤©"""
    
    # æ·»åŠ æ¿å—åˆ†æ
    if sector_analysis:
        context += f"""
[çƒ­é—¨æ¿å—åˆ†æ]"""
        for sector, data in sector_analysis.items():
            if data['total'] > 0:
                context += f"""
- {sector}: {data['zt_count']}æ¶¨åœ/{data['dt_count']}è·Œåœ"""
    
    # æ·»åŠ é£é™©æç¤º
    if risk_analysis['high_risk_count'] > 0:
        context += f"""
[é£é™©æç¤º]
- é«˜é£é™©è‚¡ç¥¨: {risk_analysis['high_risk_count']}åª
- é‡ç‚¹å…³æ³¨: {', '.join(risk_analysis['high_risk_stocks'][:5])}ç­‰"""
    
    # å§‹ç»ˆç›´æ¥åµŒå…¥å®Œæ•´CSVæ•°æ®ï¼Œä¾¿äºæ£€æŸ¥
    context += f"""
[å®Œæ•´CSVæ•°æ®]
{csv_content}
"""
    print(f"[Info] CSVæ•°æ®å¤§å°: {csv_size} å­—ç¬¦ï¼Œç›´æ¥åµŒå…¥æç¤ºè¯ (å…±{summary['total_records']}æ¡è®°å½•)")
    
    return context

def call_ai_analysis(csv_content, analysis_result):
    """
    è°ƒç”¨DeepSeekè¿›è¡Œå¸‚åœºåˆ†æ
    """
    if not DEEPSEEK_API_KEY or "sk-" not in DEEPSEEK_API_KEY:
        print("[Warning] æœªé…ç½® DEEPSEEK_API_KEYï¼Œè·³è¿‡ AI åˆ†æã€‚")
        return "æœªé…ç½® API Keyï¼Œæ— æ³•ç”Ÿæˆ AI æŠ¥å‘Šã€‚"

    # å‡†å¤‡AIä¸Šä¸‹æ–‡
    context = prepare_ai_context(csv_content, analysis_result)
    
    # æ„å»ºä¸“ä¸šæç¤ºè¯
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºä¸­å›½Aè‚¡å¸‚åœºçš„èµ„æ·±é‡‘èåˆ†æå¸ˆã€‚
æˆ‘å°†ä¸ºä½ æä¾›æœ€æ–°{len(analysis_result['daily_stats']) if analysis_result else 1}ä¸ªäº¤æ˜“æ—¥çš„æ¶¨åœï¼ˆLimit Upï¼‰å’Œè·Œåœï¼ˆLimit Downï¼‰æ•°æ®ã€‚

æ•°æ®æ ¼å¼ï¼šCSVæ ¼å¼ï¼ŒåŒ…å«è¡¨å¤´ï¼ˆæ—¥æœŸã€ç±»å‹ã€è‚¡ç¥¨ä»£ç ã€è‚¡ç¥¨åç§°ã€æœ€æ–°ä»·ã€æ¶¨è·Œå¹…ã€æŒ¯å¹…ã€æˆäº¤é¢ã€æ¢æ‰‹ç‡ã€è¿æ¿å¤©æ•°ã€å°å•æ—¶é—´ç­‰ï¼‰ã€‚

{context}

è¯·æ ¹æ®æä¾›çš„æ•°æ®ç”Ÿæˆä¸€ä»½å…¨é¢çš„å¸‚åœºåˆ†ææŠ¥å‘Šï¼Œæ¶µç›–ä»¥ä¸‹æ–¹é¢ï¼š
ğŸ“ˆ **å¸‚åœºæƒ…ç»ªè¶‹åŠ¿**ï¼šåˆ†ææ¯æ—¥æ¶¨åœä¸è·Œåœè‚¡ç¥¨æ•°é‡çš„å˜åŒ–è¶‹åŠ¿ã€‚å¸‚åœºæƒ…ç»ªæ˜¯åœ¨å›æš–ğŸ”¥è¿˜æ˜¯é™æ¸©â„ï¸ï¼Ÿ
ğŸ”¥ **çŸ­çº¿ç‚’ä½œçƒ­åº¦**ï¼šå…³æ³¨"è¿æ¿å¤©æ•°"ã€‚æ˜¯å¦æœ‰é«˜ä½è¿æ¿è‚¡ï¼ˆå¦–è‚¡/é¾™å¤´ï¼‰å‡ºç°ï¼Ÿç›®å‰å¸‚åœºçš„ç©ºé—´æ¿é«˜åº¦æ˜¯å¤šå°‘ï¼Ÿ
ğŸ¯ **çƒ­é—¨æ¿å—**ï¼šæ ¹æ®è‚¡ç¥¨åç§°ï¼ˆç»“åˆä½ å¯¹Aè‚¡æ¿å—çš„äº†è§£ï¼‰ï¼Œè¯†åˆ«å½“å‰æ´»è·ƒçš„é¢˜ææˆ–æ¦‚å¿µï¼ˆå¦‚äººå·¥æ™ºèƒ½ã€æˆ¿åœ°äº§ã€æ¶ˆè´¹ç­‰ï¼‰ã€‚
âš ï¸ **é£é™©æç¤º**ï¼šå¦‚æœè·Œåœæ•°é‡å¢åŠ æˆ–é«˜ä½è‚¡å‡ºç°äºé’±æ•ˆåº”ï¼ˆæ ¸æŒ‰é’®ï¼‰ï¼Œè¯·ç»™å‡ºé£é™©è­¦ç¤ºã€‚
ğŸ“Š **æ€»ç»“ä¸å±•æœ›**ï¼šå¯¹å½“å‰å¸‚åœºé˜¶æ®µè¿›è¡Œç®€è¦æ€»ç»“ã€‚
ğŸ’¡ **æŠ•èµ„å»ºè®®**ï¼šåŸºäºä¸Šè¿°åˆ†æï¼Œç»™å‡ºé’ˆå¯¹çŸ­çº¿é€‰æ‰‹å’Œç¨³å¥å‹æŠ•èµ„è€…çš„å…·ä½“æŠ•èµ„å»ºè®®ï¼ˆä¾‹å¦‚ï¼šä»“ä½ç®¡ç†ã€æ–¹å‘é€‰æ‹©ã€å›é¿æ¿å—ç­‰ï¼‰ã€‚

è¯·ç›´æ¥è¾“å‡ºä¸­æ–‡æŠ¥å‘Šã€‚
"""

    # æ‰“å°å®Œæ•´æç¤ºè¯ç”¨äºæ£€æŸ¥ï¼ŒCSVæ•°æ®è¿‡å¤šæ—¶åªæ˜¾ç¤ºéƒ¨åˆ†
    lines = csv_content.strip().split('\n')
    print("-" * 20 + " æç¤ºè¯å†…å®¹å¼€å§‹(æ˜¾ç¤ºCSVæ ·ä¾‹) " + "-" * 20)
    if len(lines) <= 20:
        # æ•°æ®é‡å°‘æ—¶æ˜¾ç¤ºå®Œæ•´å†…å®¹
        print(prompt)
    else:
        # æ•°æ®é‡å¤§æ—¶åªæ˜¾ç¤ºCSVå‰åéƒ¨åˆ†æ ·ä¾‹
        # æ„å»ºæ˜¾ç¤ºç‰ˆæœ¬çš„prompt
        head = '\n'.join(lines[:11])  # è¡¨å¤´+å‰10è¡Œæ•°æ®
        tail = '\n'.join(lines[-10:])  # å10è¡Œæ•°æ®
        
        # åˆ›å»ºçœç•¥ç‰ˆæœ¬çš„CSVå†…å®¹
        omit_csv = head + '\n...(ä¸­é—´çœç•¥ {} è¡Œ)...\n'.format(len(lines) - 20) + tail
        
        # æ›¿æ¢promptä¸­çš„å®Œæ•´CSVä¸ºçœç•¥ç‰ˆæœ¬
        display_prompt = prompt.replace(csv_content, omit_csv)
        print(display_prompt)
    print("-" * 20 + " æç¤ºè¯å†…å®¹ç»“æŸ " + "-" * 20)

    print("\n=== å‘ DeepSeek API å‘é€è¯·æ±‚ ===")
    
    # ä½¿ç”¨requestsç›´æ¥è°ƒç”¨DeepSeek API
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„é‡‘èåˆ†æå¸ˆã€‚"},
            {"role": "user", "content": prompt}
        ],
        "stream": False
    }

    try:
        response = requests.post(
            f"{DEEPSEEK_BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                report = result['choices'][0]['message']['content']
                return report
            else:
                print(f"DeepSeek API è¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                return "DeepSeek API è¿”å›æ ¼å¼å¼‚å¸¸"
        else:
            print(f"DeepSeek API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return f"DeepSeek API è¯·æ±‚å¤±è´¥: {response.status_code}"
            
    except Exception as e:
        print(f"è°ƒç”¨ DeepSeek API å‡ºé”™: {e}")
        return f"AI åˆ†æå¤±è´¥: {e}"

# ================= æ¶ˆæ¯æ¨é€æ¨¡å— =================

def send_push(title, content):
    """
    ä½¿ç”¨ WxPusher æ¨é€æ¶ˆæ¯
    """
    print("\n" + "="*20 + f" PUSH: {title} " + "="*20)
    print("æ­£åœ¨å‘é€ WxPusher æ¨é€...")
    print("="*50 + "\n")
    
    payload = {
        "appToken": WXPUSHER_APP_TOKEN,
        "content": content,
        "summary": title, # æ¶ˆæ¯æ‘˜è¦ï¼Œæ˜¾ç¤ºåœ¨åˆ—è¡¨é¡µ
        "contentType": 3, # 3 è¡¨ç¤º Markdown
        "topicIds": WXPUSHER_TOPIC_IDS,
        "verifyPay": False
    }
    
    try:
        response = requests.post(WXPUSHER_URL, json=payload, timeout=10)
        resp_json = response.json()
        if response.status_code == 200 and resp_json.get('code') == 1000:
            print(f"[Info] WxPusher æ¨é€æˆåŠŸ: {resp_json.get('msg')}")
        else:
            print(f"[Error] WxPusher æ¨é€å¤±è´¥: {resp_json}")
    except Exception as e:
        print(f"[Error] WxPusher è¯·æ±‚å¼‚å¸¸: {e}")

# ================= ä¸»ç¨‹åº =================

def main(days=DEFAULT_DAYS, enable_push=ENABLE_PUSH):
    """
    ä¸»åˆ†ææµç¨‹
    å€Ÿé‰´AIå¸‚åœºå®½åº¦åˆ†æå¸ˆçš„ä¸»ç¨‹åºç»“æ„
    """
    beijing_time = get_beijing_time()
    print(f"[{beijing_time.strftime('%H:%M:%S')}] å¼€å§‹æ‰§è¡Œæ¶¨åœè·Œåœå¸‚åœºåˆ†æä»»åŠ¡...")
    
    # 1. è·å–æ•°æ®
    csv_file, csv_content = fetch_market_data(days=days)
    if not csv_file or not csv_content:
        print("[Error] æ•°æ®è·å–å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return

    # 2. æ•°æ®é¢„åˆ†æ
    print(f"[{get_beijing_time().strftime('%H:%M:%S')}] æ­£åœ¨è¿›è¡Œæ•°æ®é¢„åˆ†æ...")
    analysis_result = analyze_market_structure(csv_content, days=days)
    if not analysis_result:
        print("[Error] æ•°æ®é¢„åˆ†æå¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return

    # 3. AIåˆ†æ
    print(f"[{get_beijing_time().strftime('%H:%M:%S')}] æ­£åœ¨è¯·æ±‚ DeepSeek è¿›è¡Œæ·±åº¦åˆ†æ...")
    ai_report = call_ai_analysis(csv_content, analysis_result)

    # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    beijing_time = get_beijing_time()
    report_header = f"""
> **æ¨é€æ—¶é—´**: {beijing_time.strftime('%Y-%m-%d %H:%M')} (åŒ—äº¬æ—¶é—´)
> **åˆ†æå‘¨æœŸ**: {analysis_result['summary']['date_range']}
> **æ•°æ®æ¥æº**: æ¶¨è·Œåœè‚¡ç¥¨æ˜ç»†
> **åˆ†ææ–¹æ³•**: AIæ·±åº¦åˆ†æ + ç»“æ„åŒ–æ•°æ®è§£æ

---
"""
    
    final_report = report_header + ai_report + f"""

---
*æ•°æ®æ¥æº: æ¶¨åœè·Œåœç»Ÿè®¡ | AI åˆ†æ: DeepSeek*
    """
    
    # 5. ä¿å­˜æŠ¥å‘Š
    filename = f"zt_dt_analysis_report_{beijing_time.strftime('%Y%m%d')}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(final_report)
    print(f"[Info] æŠ¥å‘Šå·²ä¿å­˜è‡³ {filename}")
    
    # 6. æ§åˆ¶å°è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*50)
    print("æ¶¨åœè·Œåœå¸‚åœºåˆ†ææŠ¥å‘Š")
    print("="*50 + "\n")
    print(ai_report)
    
    # 7. æ¨é€æ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
    if enable_push:
        push_title = f"Aè‚¡æ¶¨è·Œåœåˆ†æ ({beijing_time.strftime('%Y-%m-%d')})"
        send_push(push_title, final_report)

# ç¨‹åºç›´æ¥è¿è¡Œä¸»å‡½æ•°
main(days=DEFAULT_DAYS, enable_push=ENABLE_PUSH)
