import requests
from bs4 import BeautifulSoup
import datetime
import re
import os
import subprocess

try:
    from zoneinfo import ZoneInfo
except ImportError:
    from backports.zoneinfo import ZoneInfo

# ä»ç¯å¢ƒå˜é‡ä¸­è·å– wxpusher é…ç½®
APP_TOKEN = os.environ.get('APP_TOKEN') or "YOUR_APP_TOKEN"  # æ›¿æ¢ä½ çš„APP_TOKEN
BASE_URL = "https://wxpusher.zjiecode.com/api"
TARGET_TOPIC_IDS_STR = os.environ.get("WXPUSHER_TOPIC_IDS")
if TARGET_TOPIC_IDS_STR:
    TARGET_TOPIC_IDS = [int(x) for x in TARGET_TOPIC_IDS_STR.split(',')]
else:
    TARGET_TOPIC_IDS = [32393]  # è¯·æ›¿æ¢ä¸ºä½ çš„ Topic ID

# æ›¿æ¢ä½ çš„UID
UID = os.environ.get("WXPUSHER_UID") or "YOUR_UID"  # è¯·æ›¿æ¢ä¸ºä½ çš„ UID

# è®¾ç½®åŒ—äº¬æ—¶é—´
BEIJING_TZ = ZoneInfo("Asia/Shanghai")


def send_message(content, uids=None, topic_ids=None, summary=None, content_type=3, url=None, verify_pay_type=0):
    """å‘é€å¾®ä¿¡æ¶ˆæ¯"""
    data = {
        "appToken": APP_TOKEN,
        "content": content,
        "contentType": content_type,  # ä½¿ç”¨ Markdown æ ¼å¼
        "verifyPayType": verify_pay_type
    }
    if uids:
        data["uids"] = uids
    if topic_ids:
        data["topicIds"] = topic_ids
    if summary:
        data["summary"] = summary
    if url:
        data["url"] = url

    response = requests.post(f"{BASE_URL}/send/message", json=data)
    return response.json()


def get_m3u8_link(detail_url, title):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
        }
        response = requests.get(detail_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        match = re.search(r"æ›´æ–°è‡³(\d+)é›†", title)
        if match:
            episode_num = match.group(1)
            links = []
            for a_tag in soup.find_all('a', class_='copy_text', target='_blank'):
                text = a_tag.get_text()
                if re.search(rf"ç¬¬{episode_num}é›†", text):  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é›†æ•°
                    href = a_tag.get('href')
                    if href.startswith(('http://', 'https://')):
                        links.append(href)
                    elif href:  # å¤„ç†ç›¸å¯¹é“¾æ¥
                        links.append("https://www.moduzy.cc" + href)
            return links if links else None
        else:
            print(f"æ— æ³•ä»æ ‡é¢˜ '{title}' ä¸­æå–é›†æ•°")
            return None

    except requests.exceptions.RequestException as e:
        print(f"è·å– {detail_url} çš„é“¾æ¥å¤±è´¥: {e}")
        return None


def get_anime_updates():
    keywords = ["å®Œç¾ä¸–ç•Œ", "ä»™é€†", "åå™¬æ˜Ÿç©º", "æ–—ç ´è‹ç©¹", "æ–—ç½—å¤§é™†2", "é®å¤©", "æ­¦ç¥ä¸»å®°", "è¯›ä»™", "ç‹¬æ­¥é€é¥", "ä¸‡ç•Œç‹¬å°Š", "çµå‰‘å°Š", "å‰‘æ¥", "èµ˜å©¿", "æ˜Ÿè¾°å˜", "æ­¦åŠ¨ä¹¾å¤"]
    exact_titles = ["æ°¸ç”Ÿä¹‹æµ·å™¬ä»™çµ", "å‡¡äººä¿®ä»™ä¼ ", "çœ·æ€é‡"]
    today = datetime.datetime.now(BEIJING_TZ).date()
    valid_dates = [(today - datetime.timedelta(days=i)) for i in range(7)]
    base_url = "https://www.moduzy.cc/list1/"
    updates = []

    for page in range(1, 6):
        url = f"{base_url}?page={page}"
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            table = soup.find('table')
            if table:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) > 0:
                        title = cells[0].text.strip()
                        update_date = cells[2].text.strip()

                        if ((title in exact_titles) or any(keyword in title for keyword in keywords)) and update_date in [date.strftime('%Y-%m-%d') for date in valid_dates]:
                            original_link = row.find('a')['href']
                            detail_link = f"https://www.moduzy.cc{original_link}" if not original_link.startswith(
                                ('http://', 'https://')) else original_link
                            m3u8_links = get_m3u8_link(detail_link, title)

                            if m3u8_links:
                                update_date_obj = datetime.datetime.strptime(update_date, "%Y-%m-%d").replace(
                                    tzinfo=BEIJING_TZ)
                                weekday_zh = "å‘¨" + "ä¸€äºŒä¸‰å››äº”å…­æ—¥"[update_date_obj.weekday()]
                                match = re.search(r"æ›´æ–°è‡³(\d+)é›†", title)

                                if match:
                                    title = title.replace(match.group(0), "")
                                    update_text = f"<span style='font-size: 30px;'><strong><span style='color: {'red' if update_date_obj.date() == today else 'orange'};'> {title} \n </span></strong></span><span style='font-size: 20px;'><strong><span style='color: {'red' if update_date_obj.date() == today else 'orange'};'> ç¬¬{match.group(1)}é›† </span></strong></span>{'ğŸ”¥' if update_date_obj.date() == today else ''} ğŸ”¥æ›´æ–°æ—¥æœŸï¼š{update_date_obj.strftime('%Y-%m-%d')} {weekday_zh}\n"
                                else:
                                    update_text = f"<span style='font-size: 30px;'><strong><span style='color: {'red' if update_date_obj.date() == today else 'orange'};'> {title} </span></strong></span>\n{'ğŸ”¥ğŸ”¥' if update_date_obj.date() == today else ''} æ›´æ–°æ—¥æœŸï¼š{update_date_obj.strftime('%Y-%m-%d')} {weekday_zh}\n"

                                for link in m3u8_links:
                                    update_text += f"<a href='{link}' target='_blank'>é­”éƒ½é“¾æ¥</a>            "
                                    update_text += f"<a href='alook://{link}' target='_blank'>Alookæ‰“å¼€</a>            "
                                    update_text += "        "  # è®¾ç½®é—´éš”
                                update_text += f"<a href='{detail_link}' target='_blank'>è¯¦æƒ…é¡µ</a>\n\n"
                                updates.append(update_text)

            else:
                print(f"ç¬¬{page}é¡µæœªæ‰¾åˆ°åŒ…å«åŠ¨æ¼«ä¿¡æ¯çš„è¡¨æ ¼")

        except requests.exceptions.RequestException as e:
            print(f"è·å–ç¬¬ {page} é¡µæ•°æ®å¤±è´¥: {e}")

    return updates


def update_readme(content):
    """å°†å†…å®¹æ›´æ–°åˆ° README.md æ–‡ä»¶"""
    try:
        with open("README.md", "w") as f:
            f.write(content)

        subprocess.run(["git", "add", "README.md"], check=True)
        subprocess.run(["git", "commit", "-m", "Update README"], check=True)
        subprocess.run(["git", "push"], check=True)

        print("README.md æ›´æ–°æˆåŠŸï¼")

    except Exception as e:
        print(f"README.md æ›´æ–°å¤±è´¥ï¼š{e}")


if __name__ == "__main__":
    updates = get_anime_updates()
    if updates:
        message = f"<center><span style='font-size: 24px;'><strong><span style='color: red;'>ğŸ”¥ æœ¬å‘¨åŠ¨æ¼«æ›´æ–° ğŸ”¥</span></strong></span></center>\n\n\n" \
                  + "".join(updates)

        response = send_message(message, topic_ids=TARGET_TOPIC_IDS)
        print(response)

        update_readme(message)

    else:
        print("ä»Šæ—¥æ— æ›´æ–°")
